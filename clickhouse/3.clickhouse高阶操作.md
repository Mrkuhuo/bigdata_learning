### 1. 物化视图

这个例子演示了如何创建一个物化视图，然后如何将第二个物化视图级联到第一个物化视图上。在本页中，您将看到如何做到这一点，许多可能性，以及局限性。不同的用例可以通过使用第二个Materialized视图作为源来创建一个Materialized视图。

举例：

我们将使用一个假数据集，其中包含一组域名的每小时浏览次数。

我们的目标

1. 我们需要每个域名按月汇总的数据，
2. 我们还需要每个域名按年聚合的数据。
您可以选择以下选项之一:编写将在SELECT请求期间读取和聚合数据的查询，在摄取数据时将数据准备为一种新格式。

使用Materialized view准备数据将允许您限制ClickHouse需要做的数据和计算量，使您的SELECT请求更快。

#### 物化视图的源表

创建源表，因为我们的目标是报告聚合的数据，而不是单独的行，所以我们可以解析它，将信息传递给Materialized Views，并丢弃实际传入的数据。这满足了我们的目标并节省了存储空间，因此我们将使用Null表引擎。

```
CREATE DATABASE IF NOT EXISTS analytics;
```
```
CREATE TABLE analytics.hourly_data
(
`domain_name` String,
`event_time` DateTime,
`count_views` UInt64
)
ENGINE = Null
```
**请注意 ：可以在Null表上创建物化视图。因此写入表的数据最终会影响视图，但原始数据仍然会被丢弃。**

#### 月度聚合表和实体化视图

对于第一个物化视图，我们需要创建目标表，在本例中，它将用于分析。Monthly_aggregated_data，我们将按月份和域名存储视图的总和。

```
CREATE TABLE analytics.monthly_aggregated_data
(
`domain_name` String,
`month` Date,
`sumCountViews` AggregateFunction(sum, UInt64)
)
ENGINE = AggregatingMergeTree
ORDER BY (domain_name, month)
```

物化视图将转发目标表上的数据，看起来像这样:

```
CREATE MATERIALIZED VIEW analytics.monthly_aggregated_data_mv
TO analytics.monthly_aggregated_data
AS
SELECT
toDate(toStartOfMonth(event_time)) AS month,
domain_name,
sumState(count_views) AS sumCountViews
FROM analytics.hourly_data
GROUP BY
domain_name,
month
```

####  年度聚合表和实体化视图

现在我们将创建第二个Materialized视图，它将链接到前面的目标表monthly_aggregated_data。

首先，我们将创建一个新的目标表，该表将存储每个域名按年聚合的视图总和。

```
CREATE TABLE analytics.year_aggregated_data
(
`domain_name` String,
`year` UInt16,
`sumCountViews` UInt64
)
ENGINE = SummingMergeTree()
ORDER BY (domain_name, year)
```

此步骤定义级联。FROM语句将使用monthly_aggregated_data表，这意味着数据流将是:

1. 数据进入hourly_data表。
2. ClickHouse将接收到的数据转发到第一个物化视图monthly_aggregated_data表，
3. 最后，第2步中接收到的数据将被转发到year_aggregated_data。

```
CREATE MATERIALIZED VIEW analytics.year_aggregated_data_mv
TO analytics.year_aggregated_data
AS
SELECT
toYear(toStartOfYear(month)) AS year,
domain_name,
sumMerge(sumCountViews) as sumCountViews
FROM analytics.monthly_aggregated_data
GROUP BY
domain_name,
year
```

**请注意**
在使用物化视图时，一个常见的误解是数据是从表中读取的，这不是物化视图的工作方式;转发的数据是插入的数据块，而不是表中的最终结果。
让我们想象在这个例子中，monthly_aggregated_data中使用的引擎是一个折叠合并树，转发到我们的第二个物化视图year_aggregated_data的数据将不是折叠表的最终结果，它将转发包含SELECT…集团。
如果你正在使用折叠合并树，替换合并树，甚至SummingMergeTree，并且你计划创建一个级联物化视图，你需要理解这里描述的限制。

#### 样本数据

现在是时候通过插入一些数据来测试我们的级联物化视图了:

```
INSERT INTO analytics.hourly_data (domain_name, event_time, count_views)
VALUES ('clickhouse.com', '2019-01-01 10:00:00', 1),
('clickhouse.com', '2019-02-02 00:00:00', 2),
('clickhouse.com', '2019-02-01 00:00:00', 3),
('clickhouse.com', '2020-01-01 00:00:00', 6);
```

如果您选择分析的内容。hourly_data您将看到以下内容，因为表引擎是Null，但数据已被处理。

```
SELECT * FROM analytics.hourly_data
```
```
Ok.

0 rows in set. Elapsed: 0.002 sec.
```

我们使用了一个小数据集，以确保我们可以跟踪并将结果与我们预期的结果进行比较，一旦你的流程在一个小数据集上是正确的，你就可以转移到大量的数据。

#### 结果

如果尝试通过选择sumCountViews字段来查询目标表，则会看到二进制表示(在某些终端中)，因为值不是存储为数字，而是存储为AggregateFunction类型。要获得聚合的最终结果，您应该使用-Merge后缀。

你可以通过这个查询看到AggregateFunction中存储的特殊字符:

```
┌─sumCountViews─┐
│               │
│               │
│               │
└───────────────┘

3 rows in set. Elapsed: 0.003 sec.
```

相反，让我们尝试使用Merge后缀来获取sumCountViews值:

```
SELECT
sumMerge(sumCountViews) as sumCountViews
FROM analytics.monthly_aggregated_data;
```
```
┌─sumCountViews─┐
│            12 │
└───────────────┘

1 row in set. Elapsed: 0.003 sec.
```
在AggregatingMergeTree中，我们将AggregateFunction定义为sum，因此可以使用sumMerge。当我们在AggregateFunction上使用函数avg时，我们将使用avgMerge，依此类推。
```
SELECT
month,
domain_name,
sumMerge(sumCountViews) as sumCountViews
FROM analytics.monthly_aggregated_data
GROUP BY
domain_name,
month
```
现在我们可以回顾一下物化视图回答了我们所定义的目标。

现在我们已经将数据存储在目标表monthly_aggregated_data中，我们可以获得每个域名按月聚合的数据:
```
SELECT
month,
domain_name,
sumMerge(sumCountViews) as sumCountViews
FROM analytics.monthly_aggregated_data
GROUP BY
domain_name,
month
```
```
┌──────month─┬─domain_name────┬─sumCountViews─┐
│ 2020-01-01 │ clickhouse.com │             6 │
│ 2019-01-01 │ clickhouse.com │             1 │
│ 2019-02-01 │ clickhouse.com │             5 │
└────────────┴────────────────┴───────────────┘

3 rows in set. Elapsed: 0.004 sec.
```

每个域名按年汇总的数据:
```
SELECT
year,
domain_name,
sum(sumCountViews)
FROM analytics.year_aggregated_data
GROUP BY
domain_name,
year
```
```
┌─year─┬─domain_name────┬─sum(sumCountViews)─┐
│ 2019 │ clickhouse.com │                  6 │
│ 2020 │ clickhouse.com │                  6 │
└──────┴────────────────┴────────────────────┘

2 rows in set. Elapsed: 0.004 sec.
```

### 2. 使用TTL(生存时间)管理数据

#### TTL概述

TTL(生存时间)指的是在经过一定时间间隔后移动、删除或卷起行或列的能力。虽然“生存时间”听起来似乎只适用于删除旧数据，但TTL有几个用例:

* 删除旧数据:毫无疑问，您可以在指定的时间间隔后删除行或列
* 在磁盘之间移动数据:经过一段时间后，您可以在存储卷之间移动数据——这对于部署热/暖/冷架构非常有用
* 数据汇总:在删除旧数据之前，将旧数据汇总成各种有用的聚合和计算

**请注意：TTL可以应用于整个表或特定列。**

#### TTL语法

TTL子句可以出现在列定义之后和/或表定义的末尾。使用INTERVAL子句定义时间长度(需要为Date或DateTime数据类型)。例如，下表有两列TTL子句:

```
CREATE TABLE example1 (
timestamp DateTime,
x UInt32 TTL now() + INTERVAL 1 MONTH,
y String TTL timestamp + INTERVAL 1 DAY,
z String
)
ENGINE = MergeTree
ORDER BY tuple()
```

* x列的生存时间是1个月后
* y列距离时间戳列的时间为1天:
* 当间隔失效时，列过期。ClickHouse将列值替换为其数据类型的默认值。如果数据部分中的所有列值过期，ClickHouse将从文件系统中的数据部分删除该列。

请注意:TTL规则可以被修改或删除。有关更多详细信息，请参阅表TTL的操作页面。

#### 触发TTL事件

过期行的删除或聚合不是立即的——它只在表合并期间发生。如果你有一个表没有主动合并(无论出于什么原因)，有两个设置会触发TTL事件:

* merge_with_ttl_timeout:在重复merge和delete TTL之前的最小延迟(秒)。缺省值是14400秒(4小时)。
* merge_with_recompression_ttl_timeout:在使用重新压缩TTL(在删除数据之前卷起数据的规则)重复合并之前的最小延迟(以秒为单位)。缺省值:14400秒(4小时)。

因此，默认情况下，至少每4小时将TTL规则应用到表一次。如果需要更频繁地应用TTL规则，只需修改上面的设置。

#### 删除行

要在一段时间后从表中删除整个行，请在表级别定义TTL规则:

```
CREATE TABLE customers (
timestamp DateTime,
name String,
balance Int32,
address String
)
ENGINE = MergeTree
ORDER BY timestamp
TTL timestamp + INTERVAL 12 HOUR
```

#### 删除列

假设您不删除整个行，只希望余额列和地址列过期。让我们修改customers表，并为两个列添加一个TTL为2小时:

```
ALTER TABLE customers
MODIFY COLUMN balance Int32 TTL timestamp + INTERVAL 2 HOUR,
MODIFY COLUMN address String TTL timestamp + INTERVAL 2 HOUR
```

#### 实现汇总

假设我们希望在一段时间后删除行，但保留一些数据用于报告。我们不需要所有的细节——只需要历史数据的一些聚合结果。这可以通过向TTL表达式中添加GROUP by子句以及表中的一些列来实现，以存储聚合的结果。

假设在下面的命中表中，我们希望删除旧行，但在删除行之前保留命中列的和和最大值。我们将需要一个字段来存储这些值，并且我们将需要在TTL子句中添加一个GROUP BY子句，该子句将总和和最大值卷起来:

```
CREATE TABLE hits (
timestamp DateTime,
id String,
hits Int32,
max_hits Int32 DEFAULT hits,
sum_hits Int64 DEFAULT hits
)
ENGINE = MergeTree
PRIMARY KEY (id, toStartOfDay(timestamp), timestamp)
TTL timestamp + INTERVAL 1 DAY
GROUP BY id, toStartOfDay(timestamp)
SET
max_hits = max(max_hits),
sum_hits = sum(sum_hits);
```

点击表上的一些注意事项:

* TTL子句中的GROUP BY列必须是PRIMARY KEY的前缀，并且我们希望根据一天的开始对结果进行分组。因此，将toStartOfDay(timestamp)添加到主键
* 我们添加了两个字段来存储聚合的结果:max_hits和sum_hits
* 根据SET子句的定义方式，将max_hits和sum_hits的默认值设置为hits对于我们的逻辑工作是必要的

#### 实现热/暖/冷架构

在处理大量数据时，一种常见的做法是随着数据的老化而移动数据。下面是使用TTL命令的TO DISK和TO VOLUME子句在ClickHouse中实现热/暖/冷架构的步骤。(顺便说一下，这并不一定是一个冷热的事情——你可以使用TTL来移动数据，无论你有什么用例。)

TO DISK和TO VOLUME选项是指在ClickHouse配置文件中定义的磁盘或卷的名称。创建一个名为my_system.xml(或任何文件名)的新文件来定义磁盘，然后定义使用磁盘的卷。将XML文件放在/etc/clickhouse-server/config.d目录下 使配置应用到您的系统:

```
<clickhouse>
    <storage_configuration>
        <disks>
            <default>
            </default>
           <hot_disk>
              <path>./hot/</path>
           </hot_disk>
           <warm_disk>
              <path>./warm/</path>
           </warm_disk>
           <cold_disk>
              <path>./cold/</path>
           </cold_disk>
        </disks>
        <policies>
            <default>
                <volumes>
                    <default>
                        <disk>default</disk>
                    </default>
                    <hot_volume>
                        <disk>hot_disk</disk>
                    </hot_volume>
                    <warm_volume>
                        <disk>warm_disk</disk>
                    </warm_volume>
                    <cold_volume>
                        <disk>cold_disk</disk>
                    </cold_volume>
                </volumes>
            </default>
        </policies>
    </storage_configuration>
</clickhouse>
```

上面的配置指的是三个磁盘，它们指向ClickHouse可以读写的文件夹。卷可以包含一个或多个磁盘—我们为三个磁盘中的每个磁盘定义了一个卷。让我们来查看磁盘:

```
SELECT name, path, free_space, total_space
FROM system.disks
```
```
┌─name────────┬─path───────────┬───free_space─┬──total_space─┐
│ cold_disk   │ ./data/cold/   │ 179143311360 │ 494384795648 │
│ default     │ ./             │ 179143311360 │ 494384795648 │
│ hot_disk    │ ./data/hot/    │ 179143311360 │ 494384795648 │
│ medium_disk │ ./data/medium/ │ 179143311360 │ 494384795648 │
└─────────────┴────────────────┴──────────────┴──────────────┘
```

让我们来验证一下卷:
```
SELECT
volume_name,
disks
FROM system.storage_policies
```
```
┌─volume_name─┬─disks─────────┐
│ default     │ ['default']   │
│ hot_volume  │ ['hot_disk']  │
│ warm_volume │ ['warm_disk'] │
│ cold_volume │ ['cold_disk'] │
└─────────────┴───────────────┘
```
现在，我们将添加一个TTL规则，在热卷、热卷和冷卷之间移动数据:
```
ALTER TABLE my_table
MODIFY TTL
trade_date TO VOLUME 'hot_volume',
trade_date + INTERVAL 2 YEAR TO VOLUME 'warm_volume',
trade_date + INTERVAL 4 YEAR TO VOLUME 'cold_volume';
```
新的TTL规则应该会实现，但你可以强制它以实现:
```
ALTER TABLE my_table
MATERIALIZE TTL
```
验证您的数据已通过系统移动到预期的磁盘。零件表:
```
Using the system.parts table, view which disks the parts are on for the crypto_prices table:

SELECT
name,
disk_name
FROM system.parts
WHERE (table = 'my_table') AND (active = 1)
```
响应如下所示:
```
┌─name────────┬─disk_name─┐
│ all_1_3_1_5 │ warm_disk │
│ all_2_2_0   │ hot_disk  │
└─────────────┴───────────┘
```

### 3. 重复数据删除策略

用于Upserts和频繁更新的行级重复数据删除策略

重复数据删除是指删除数据集中重复行的过程。在OLTP数据库中，这很容易做到，因为每一行都有唯一的主键——但代价是插入速度较慢。首先需要搜索插入的每一行，如果找到了，就需要替换。

当涉及到数据插入时，ClickHouse是为速度而构建的。存储文件是不可变的，并且ClickHouse在插入一行之前不会检查现有的主键—因此重复数据删除需要更多的工作。这也意味着重复数据删除不是立即的——而是最终的——这有一些副作用:

* 在任何时候，你的表仍然可以有重复的(具有相同排序键的行)
* 重复行的实际删除发生在部分合并期间
* 您的查询需要考虑重复的可能性

#### 重复数据删除选项

重复数据删除在ClickHouse中使用以下表引擎实现:

1. ReplacingMergeTree表引擎:使用这个表引擎，在合并过程中删除具有相同排序键的重复行。ReplacingMergeTree是模拟upsert行为(您希望查询返回插入的最后一行)的一个很好的选择。
2. 折叠行:CollapsingMergeTree和VersionedCollapsingMergeTree表引擎使用“取消”现有行并插入新行的逻辑。它们的实现要比ReplacingMergeTree复杂得多，但是您的查询和聚合可以更简单地编写，而不必担心数据是否已经合并。当需要频繁更新数据时，这两个表引擎非常有用。

下面我们将介绍这两种技术。有关更多详细信息，请查看我们免费的按需重复数据删除培训课程。

#### 为Upserts使用ReplacingMergeTree

让我们看一个简单的例子，其中一个表包含Hacker News评论，其中的views列表示一条评论被浏览的次数。假设我们在一篇文章发布时插入一个新行，如果该值增加，我们每天用总浏览量插入一个新行:

```
CREATE TABLE hackernews_rmt (
id UInt32,
author String,
comment String,
views UInt64
)
ENGINE = ReplacingMergeTree
PRIMARY KEY (author, id)
```

让我们插入两行:

```
INSERT INTO hackernews_rmt VALUES
(1, 'ricardo', 'This is post #1', 0),
(2, 'ch_fan', 'This is post #2', 0)
```

要更新views列，插入一个具有相同主键的新行(注意views列的新值):

```
INSERT INTO hackernews_rmt VALUES
(1, 'ricardo', 'This is post #1', 100),
(2, 'ch_fan', 'This is post #2', 200)
```

这个表现在有4行:

```
SELECT *
FROM hackernews_rmt
```
```
┌─id─┬─author──┬─comment─────────┬─views─┐
│  2 │ ch_fan  │ This is post #2 │     0 │
│  1 │ ricardo │ This is post #1 │     0 │
└────┴─────────┴─────────────────┴───────┘
┌─id─┬─author──┬─comment─────────┬─views─┐
│  2 │ ch_fan  │ This is post #2 │   200 │
│  1 │ ricardo │ This is post #1 │   100 │
└────┴─────────┴─────────────────┴───────┘
```
上面输出中的独立框演示了幕后的两部分——这些数据还没有合并，所以重复的行还没有被删除。让我们在SELECT查询中使用FINAL关键字，这将导致查询结果的逻辑合并:
```
SELECT *
FROM hackernews_rmt
FINAL
```
```
┌─id─┬─author──┬─comment─────────┬─views─┐
│  2 │ ch_fan  │ This is post #2 │   200 │
│  1 │ ricardo │ This is post #1 │   100 │
└────┴─────────┴─────────────────┴───────┘
```
结果只有2行，插入的最后一行是返回的行。
**请注意：如果你有少量的数据，使用FINAL也可以。如果要处理大量数据，使用FINAL可能不是最佳选择。让我们讨论查找列的最新值的更好选择…**

#### 避免FINAL

让我们再次为这两个唯一的行更新views列:
```
INSERT INTO hackernews_rmt VALUES
(1, 'ricardo', 'This is post #1', 150),
(2, 'ch_fan', 'This is post #2', 250)
```

这个表现在有6行，因为实际的合并还没有发生(只有使用FINAL时的查询时合并)。
```
SELECT *
FROM hackernews_rmt
```
```
┌─id─┬─author──┬─comment─────────┬─views─┐
│  2 │ ch_fan  │ This is post #2 │   200 │
│  1 │ ricardo │ This is post #1 │   100 │
└────┴─────────┴─────────────────┴───────┘
┌─id─┬─author──┬─comment─────────┬─views─┐
│  2 │ ch_fan  │ This is post #2 │     0 │
│  1 │ ricardo │ This is post #1 │     0 │
└────┴─────────┴─────────────────┴───────┘
┌─id─┬─author──┬─comment─────────┬─views─┐
│  2 │ ch_fan  │ This is post #2 │   250 │
│  1 │ ricardo │ This is post #1 │   150 │
└────┴─────────┴─────────────────┴───────┘
```

我们不使用FINAL，而是使用一些业务逻辑——我们知道views列总是在增加，所以我们可以在按所需列分组后使用max函数选择值最大的行:
```
SELECT
id,
author,
comment,
max(views)
FROM hackernews_rmt
GROUP BY (id, author, comment)
```
```
┌─id─┬─author──┬─comment─────────┬─max(views)─┐
│  2 │ ch_fan  │ This is post #2 │        250 │
│  1 │ ricardo │ This is post #1 │        150 │
└────┴─────────┴─────────────────┴────────────┘
```

按照上面查询所示进行分组实际上比使用FINAL关键字更有效(就查询性能而言)。FINAL关键字强制查询在单个线程中运行，而GROUP BY则并行执行。

#### 使用CollapsingMergeTree频繁更新列

更新列涉及删除现有行并将其替换为新值。正如你已经看到的，这种类型的突变在ClickHouse最终发生-在合并期间。如果有很多行要更新，实际上可以更有效地避免使用ALTER TABLE. update，而只是将新数据插入到现有数据旁边。我们可以添加一个列来表示数据是陈旧的还是新的……实际上有一个表引擎已经很好地实现了这个行为，特别是考虑到它会自动为您删除陈旧的数据。让我们看看它是如何工作的。

假设我们使用外部系统跟踪Hacker News评论的浏览量，每隔几个小时，我们将数据推入ClickHouse。我们希望旧行被删除，新行表示每个Hacker News评论的新状态。我们可以使用一个CollapsingMergeTree来实现这个行为。

让我们定义一个表来存储视图的数量:
```
CREATE TABLE hackernews_views (
id UInt32,
author String,
views UInt64,
sign Int8
)
ENGINE = CollapsingMergeTree(sign)
PRIMARY KEY (id, author)
```
**注意，hackernews_views表有一个名为sign的Int8列，称为sign列。符号列的名称是任意的，但Int8数据类型是必需的，请注意，列名被传递给了CollapsingMergeTree表的构造函数。**

折叠合并树表的符号列是什么?它表示行的状态，符号列只能是1或-1。下面是它的工作原理:

* 如果两行有相同的主键(或排序顺序，如果它与主键不同)，但符号列的值不同，则插入+1的最后一行成为状态行，其他行相互抵消
* 在合并过程中，将删除相互抵消的行
* 没有匹配对的行将被保留

让我们向hackernews_views表中添加一行。因为它是这个主键的唯一行，所以我们将它的状态设置为1:
```
INSERT INTO hackernews_views VALUES
(123, 'ricardo', 0, 1)
```
现在假设我们想要改变视图列。插入两行:一行取消现有行，另一行包含该行的新状态:
```
INSERT INTO hackernews_views VALUES
(123, 'ricardo', 0, -1),
(123, 'ricardo', 150, 1)
```
现在表有3行主键(123，'ricardo'):
```
SELECT *
FROM hackernews_views
```
```
┌──id─┬─author──┬─views─┬─sign─┐
│ 123 │ ricardo │     0 │   -1 │
│ 123 │ ricardo │   150 │    1 │
└─────┴─────────┴───────┴──────┘
┌──id─┬─author──┬─views─┬─sign─┐
│ 123 │ ricardo │     0 │    1 │
└─────┴─────────┴───────┴──────┘
```
**注意，添加FINAL将返回当前状态行:**
```
SELECT *
FROM hackernews_views
FINAL
```
```
┌──id─┬─author──┬─views─┬─sign─┐
│ 123 │ ricardo │   150 │    1 │
└─────┴─────────┴───────┴──────┘
```
当然，对于大型表，不建议使用FINAL。

**请注意 :在我们的例子中，为views列传递的值实际上并不需要，它也不必与旧行的视图的当前值匹配。事实上，你可以用一个主键和一个-1来取消一行:**
```
INSERT INTO hackernews_views(id, author, sign) VALUES
(123, 'ricardo', -1)
```
#### 多线程实时更新

对于一个CollapsingMergeTree表，行之间使用符号列相互抵消，一行的状态由插入的最后一行决定。但是，如果您从不同的线程插入行，在这些线程中，行可以按顺序插入，那么这可能会出现问题。在这种情况下，使用“最后”行不起作用。

这就是VersionedCollapsingMergeTree派上用场的地方——它像CollapsingMergeTree一样折叠行，但它不保留插入的最后一行，而是保留您指定的版本列中具有最高值的行。

让我们看一个例子。假设我们想要跟踪Hacker News评论的浏览量，并且数据经常更新。我们希望报告使用最新的值，而不强制或等待合并。我们从一个类似于CollapsedMergeTree的表开始，除了我们添加了一个列来存储行状态的版本:
```
CREATE TABLE hackernews_views_vcmt (
id UInt32,
author String,
views UInt64,
sign Int8,
version UInt32
)
ENGINE = VersionedCollapsingMergeTree(sign, version)
PRIMARY KEY (id, author)
```
注意，该表使用VersionsedCollapsingMergeTree作为引擎，并传入符号列和版本列。以下是表的运行方式:

* 它删除具有相同主键和版本号以及不同符号的每对行
* 行插入的顺序无关紧要
* 注意，如果版本列不是主键的一部分，ClickHouse将它隐式地作为最后一个字段添加到主键

在编写查询时使用相同类型的逻辑——根据主键分组，并使用聪明的逻辑来避免已经取消但尚未删除的行。让我们在hackernews_views_vcmt表中添加一些行:
```
INSERT INTO hackernews_views_vcmt VALUES
(1, 'ricardo', 0, 1, 1),
(2, 'ch_fan', 0, 1, 1),
(3, 'kenny', 0, 1, 1)
```
现在我们更新其中两行并删除其中一行。要取消一行，请确保包含先前的版本号(因为它是主键的一部分):
```
INSERT INTO hackernews_views_vcmt VALUES
(1, 'ricardo', 0, -1, 1),
(1, 'ricardo', 50, 1, 2),
(2, 'ch_fan', 0, -1, 1),
(3, 'kenny', 0, -1, 1),
(3, 'kenny', 1000, 1, 2)
```

我们将运行与之前相同的查询，根据符号列巧妙地加减值:
```
SELECT
id,
author,
sum(views * sign)
FROM hackernews_views_vcmt
GROUP BY (id, author)
HAVING sum(sign) > 0
ORDER BY id ASC
```
结果是两行:
```
┌─id─┬─author──┬─sum(multiply(views, sign))─┐
│  1 │ ricardo │                         50 │
│  3 │ kenny   │                       1000 │
└────┴─────────┴────────────────────────────┘
```
让我们强制合并一个表:
```
OPTIMIZE TABLE hackernews_views_vcmt
```
结果中应该只有两行:
```
SELECT *
FROM hackernews_views_vcmt
```
```
┌─id─┬─author──┬─views─┬─sign─┬─version─┐
│  1 │ ricardo │    50 │    1 │       2 │
│  3 │ kenny   │  1000 │    1 │       2 │
└────┴─────────┴───────┴──────┴─────────┘
```
当您希望在插入来自多个客户端和/或线程的行时实现重复数据删除时，VersionedCollapsingMergeTree表非常方便。

